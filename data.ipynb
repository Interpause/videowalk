{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from math import floor\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "rng = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  1.13.1+cu117\n",
      "CUDA available:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"CUDA available: \", torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See [./code/utils/arguments.py](./code/utils/arguments.py) for arguments used in upstream.\n",
    "  - Upstream author was lazy and passed around the args object, so it is hard to trace the parameters of stuff.\n",
    "- Upstream author modified torchvision loader from 2020: <https://github.com/ajabri/videowalk/blob/master/code/data/kinetics.py>\n",
    "  - Current torchvision loader is now: <https://pytorch.org/vision/stable/generated/torchvision.datasets.Kinetics.html>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapVideoTransform:\n",
    "    \"\"\"Map transform over THWC tensor.\"\"\"\n",
    "\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, video):\n",
    "        return torch.stack([self.transform(frame) for frame in video])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check if its RGB or BGR order.\n",
    "# TODO: check if you missed preprocessing where THWC -> TCHW since upstream authors use THWC.\n",
    "# TODO: steal the augmentation transforms & figure out why its repeated twice.\n",
    "\n",
    "to_tensor = T.ToTensor()\n",
    "to_PIL = T.ToPILImage()\n",
    "norm_size = T.Resize((640, 640)) # works only on PIL/numpy # upstream uses 256x256\n",
    "norm_color = T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "transform = MapVideoTransform(T.Compose([to_PIL, norm_size, to_tensor, norm_color]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/datasets/video_utils.py:219: UserWarning: There aren't enough frames in the current video to get a clip for the given clip length and frames between clips. The video (and potentially others) will be skipped.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Can re-use metadata from previous pickled dataset object if need to reconfigure.\n",
    "try:\n",
    "    kinetics400 = torch.load(\"datasets/kinetics400.pt\")\n",
    "except:\n",
    "    kinetics400 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinetics400 = torchvision.datasets.Kinetics(\n",
    "  root=\"/home/jovyan/downloads/kinetics400\",\n",
    "  frames_per_clip=4,\n",
    "  num_classes=\"400\",\n",
    "  split=\"val\",\n",
    "  frame_rate=8,\n",
    "  step_between_clips=8,\n",
    "  # download=True,\n",
    "  transform=transform,\n",
    "  num_workers=16,\n",
    "  num_download_workers=16,\n",
    "  output_format=\"TCHW\",\n",
    "  _precomputed_metadata=dict(\n",
    "    video_paths=kinetics400.video_clips.video_paths,\n",
    "    video_fps=kinetics400.video_clips.video_fps,\n",
    "    video_pts=kinetics400.video_clips.video_pts\n",
    "  )\n",
    ")\n",
    "torch.save(kinetics400, \"datasets/kinetics400.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos:  19881\n",
      "Total clips:  190357\n",
      "Filtered clips:  5004\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets.samplers import RandomClipSampler\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "def collate(batch):\n",
    "  \"\"\"torchvision.datasets.video_utils.VideoClips returns metadata along with video tensor. Select video tensor & stack into batch.\"\"\"\n",
    "  # See https://github.com/pytorch/vision/blob/707457050620e1f70ab1b187dad81cc36a7f9180/torchvision/datasets/video_utils.py#L289\n",
    "  # list of (T, C, H, W) videos\n",
    "  batch = [c[0] for c in batch]\n",
    "  # let torchvision handle conversion to tensor\n",
    "  return default_collate(batch)\n",
    "\n",
    "subset_idx = torch.randperm(kinetics400.video_clips.num_videos(), generator=rng)[:5000].tolist()\n",
    "sampler = RandomClipSampler(kinetics400.video_clips.subset(subset_idx), 1)\n",
    "dataloader = DataLoader(\n",
    "    kinetics400,\n",
    "    batch_size=6,\n",
    "    sampler=sampler,\n",
    "    num_workers=16,\n",
    "    collate_fn=collate,\n",
    "    pin_memory=True,\n",
    "    generator=rng\n",
    ")\n",
    "\n",
    "print(\"Total videos: \", kinetics400.video_clips.num_videos())\n",
    "print(\"Total clips: \", len(kinetics400))\n",
    "print(\"Filtered clips: \", len(dataloader)*dataloader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader tensor shape:  torch.Size([6, 4, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(dataloader))\n",
    "print(\"Dataloader tensor shape: \", sample.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from types import SimpleNamespace\n",
    "# from videowalk.utils.visualize import Visualize\n",
    "# \n",
    "# # author was lazy\n",
    "# args = SimpleNamespace(\n",
    "#     name=\"videowalk-test\",\n",
    "#     port=80,\n",
    "#     server=\"localhost\"\n",
    "# )\n",
    "# \n",
    "# # doesnt work if no ports are available...\n",
    "# \n",
    "# viz = Visualize(args)\n",
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "stride Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "stride Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "from videowalk.model import CRW\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    dropout=0,\n",
    "    featdrop=0.0,\n",
    "    temperature=0.07,\n",
    "    head_depth=0,\n",
    "    device=device,\n",
    "    flip=False,\n",
    "    sk_targets=False,\n",
    "    model_type=\"imagenet18\",\n",
    "    remove_layers=[],\n",
    ")\n",
    "\n",
    "model = CRW(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: lr_milestones should be per total epochs, not per epoch.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_milestones = [0.2, 0.8] # 20% and 80% of epoch\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=[floor(len(dataloader) * m) for m in lr_milestones], gamma=0.1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Checkpointer:\n",
    "    model: torch.nn.Module\n",
    "    optimizer: torch.optim.Optimizer\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler\n",
    "    epoch: int\n",
    "    args: SimpleNamespace\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'lr_scheduler': self.scheduler.state_dict(),\n",
    "            'epoch': self.epoch,\n",
    "            'args': self.args\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        self.epoch = checkpoint['epoch']\n",
    "        self.args = checkpoint['args']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "checkpointer = Checkpointer(model, optimizer, lr_scheduler, 0, args)\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    print(\"Epoch: \", checkpointer.epoch)\n",
    "    for video in tqdm(dataloader):\n",
    "        video = video.to(device)\n",
    "\n",
    "        _, loss, _ = model(video)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    checkpointer.epoch += 1\n",
    "    checkpointer.save(f\"epoch{checkpointer.epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/834 [00:00<?, ?it/s]/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/opt/conda/envs/videowalk/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "100%|██████████| 834/834 [1:26:54<00:00,  6.25s/it]\n"
     ]
    }
   ],
   "source": [
    "train_one_epoch()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Encoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "stride Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "stride Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "stride Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "From3D(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): None\n",
      "    (fc): None\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from videowalk.utils import make_encoder\n",
    "\n",
    "encoder = make_encoder(args)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "From3D                                        [1, 512, 8, 8, 8]         --\n",
       "├─ResNet: 1-1                                 [8, 512, 8, 8]            --\n",
       "│    └─Conv2d: 2-1                            [8, 64, 32, 32]           9,408\n",
       "│    └─BatchNorm2d: 2-2                       [8, 64, 32, 32]           128\n",
       "│    └─ReLU: 2-3                              [8, 64, 32, 32]           --\n",
       "│    └─MaxPool2d: 2-4                         [8, 64, 16, 16]           --\n",
       "│    └─Sequential: 2-5                        [8, 64, 16, 16]           --\n",
       "│    │    └─BasicBlock: 3-1                   [8, 64, 16, 16]           73,984\n",
       "│    │    └─BasicBlock: 3-2                   [8, 64, 16, 16]           73,984\n",
       "│    └─Sequential: 2-6                        [8, 128, 8, 8]            --\n",
       "│    │    └─BasicBlock: 3-3                   [8, 128, 8, 8]            230,144\n",
       "│    │    └─BasicBlock: 3-4                   [8, 128, 8, 8]            295,424\n",
       "│    └─Sequential: 2-7                        [8, 256, 8, 8]            --\n",
       "│    │    └─BasicBlock: 3-5                   [8, 256, 8, 8]            919,040\n",
       "│    │    └─BasicBlock: 3-6                   [8, 256, 8, 8]            1,180,672\n",
       "│    └─Sequential: 2-8                        [8, 512, 8, 8]            --\n",
       "│    │    └─BasicBlock: 3-7                   [8, 512, 8, 8]            3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [8, 512, 8, 8]            4,720,640\n",
       "===============================================================================================\n",
       "Total params: 11,176,512\n",
       "Trainable params: 11,176,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.02\n",
       "===============================================================================================\n",
       "Input size (MB): 0.39\n",
       "Forward/backward pass size (MB): 53.48\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 98.58\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Dataloader output tensor: NCTHW\n",
    "summary(encoder, input_size=(1, 3, 8, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videowalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fbaa75d1bc8ffaaae769728f947949c91dc194ad4023d0896d73c2ec13a8747"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
